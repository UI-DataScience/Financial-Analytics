{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  JW Paper Replicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/JiangmengZhang/Documents/ml-finance'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "os.getcwd()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LM list + combine inflection (stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def read_lm(txt): \n",
    "    with  open(txt) as f:\n",
    "        lines = f.readlines()\n",
    "    new_lines = [x for x in lines if not x=='\\n']\n",
    "    new_lines = [x[:-1] for x in new_lines]\n",
    "    return(new_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos = read_lm(\"LM_Positive.txt\")\n",
    "neg = read_lm(\"LM_Negative.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    " from nltk.stem.porter import PorterStemmer\n",
    " porter = PorterStemmer() # could be Snowball stemmer or Lancaster stemmer\n",
    " def tokenizer_porter(lm):\n",
    "    return ([porter.stem(word) for word in lm])\n",
    " pos_stem = tokenizer_porter(pos)\n",
    " neg_stem = tokenizer_porter(neg) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmas = WordNetLemmatizer()\n",
    "def tokenizer_lemmas(lm):\n",
    "    return([lemmas.lemmatize(word.lower(), pos = 'v') for word in lm])\n",
    "pos_lem = tokenizer_lemmas(pos)\n",
    "neg_lem = tokenizer_lemmas(neg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import Counter\n",
    "def tokenize_doc(doc):\n",
    "#doc = \"A paragraph from the Ancient Greek paragraphos able excellent, to write beside or written beside) is a self-contained unit of a discourse in writing dealing with a particular point or idea. A paragraph consists of one or more sentences.\"\n",
    "    doc = doc.lower()\n",
    "    wordlist = doc.split()\n",
    "    ## remove no's neighbour\n",
    "    noidx = [i for i,x in enumerate(wordlist) if x in [\"no\", \"not\", \"never\"]]\n",
    "    noidx2 = []\n",
    "    for one in noidx:\n",
    "        noidx2.append(range(one-3, one+4))\n",
    "    noidx2 = list(set(noidx2))\n",
    "    wordlist2 = [wordlist[i] for i in range(len(wordlist)) if not i in noidx2] \n",
    "\n",
    "    ## word count\n",
    "    wordcount = Counter(wordlist2)\n",
    "    posneg = pos + neg\n",
    "    # posneg_lem = pos_lem + neg_lem\n",
    "    posneg_stem = pos_stem + neg_stem\n",
    "    posneglower = [item.lower() for item in posneg]\n",
    "    wordcount2 = {x:wordcount[x] for x in wordcount.keys() if x in posneglower}\n",
    "    wordcount2 = {x:wordcount2.get(x,0) for x in posneglower} #filling 0\n",
    "    wordcount3 = dict()\n",
    "    for i,x in enumerate(posneg_stem):\n",
    "        wordcount3[x] = wordcount3.get(x, 0) + wordcount2[posneg2[i]] # combine\n",
    "    return(len(wordlist), wordcount3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10K\n",
    "1. remove stop words and proper nouns\n",
    "2. do not count positive or negative words that are accompanied by a \"negator\" (not, no, never) with a distance of 3 words\n",
    "3. tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stock Price\n",
    "CRSP value-weight index return?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Organize all data into one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## \"release\":10K release date;  \"company\"; \"ret\": abnormal return; \"len\": # of word; \"wordlist\": tokenlized wordlist; \n",
    "df = pd.DataFrame()\n",
    "for doc in docs:\n",
    "    res = tokenize_doc(doc)\n",
    "    df = df.append(res[1].values()/res[0])\n",
    "    \n",
    "df['ret'] = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression of return on word to get weight and score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "X = df.iloc[:, :-1].values\n",
    "y = df.iloc[:, -1].values\n",
    "lm = linear_model.LinearRegression()\n",
    "lm.fit(X, y)\n",
    "lm.coef_\n",
    "\n",
    "X2 = sm.add_constant(X)\n",
    "est = sm.OLS(y, X2)\n",
    "est2 = est.fit()\n",
    "print(est2.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression of return on score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_idx = range(len(set(pos_stem)))\n",
    "neg_idx = range(len(set(pos_stem)), len(set(posneg_stem)))\n",
    "score_pos = X.iloc[:, pos_idx]*lm.coef_[pos_idx]\n",
    "score_neg = X.iloc[:, neg_idx]*lm.coef_(neg_idx)\n",
    "Xscore = pd.DateFrame()\n",
    "Kscore['pos'] = score_pos\n",
    "Xscore['neg'] = score_neg\n",
    "\n",
    "Xscore2 = sm.add_constant(X)\n",
    "est_score = sm.OLS(y, Xscore2)\n",
    "est_score2 = est_score.fit()\n",
    "print(est_score2.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Future works\n",
    "1. collocation\n",
    "2. training vs testing\n",
    "\n",
    "Question:\n",
    "1. Note my original (very old) suggestion was to use map (which is much slower):\n",
    "df1['e'] = df1['a'].map(lambda x: np.random.random())\n",
    "\n",
    "2. why red above?\n",
    "3. return 2 variables \n",
    "4. df.values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
